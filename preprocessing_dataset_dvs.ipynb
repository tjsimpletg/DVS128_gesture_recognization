{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import struct\n",
    "from typing import Callable, Dict, Optional, Tuple\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvs_path = \"/home/zhang/S2/RP/DataSet/DVS_Gesture_dataset/DvsGesture\"\n",
    "testData_path = \"/home/zhang/S2/RP/DataSet/TestData\"\n",
    "events_dvs_split_path = \"/home/zhang/S2/RP/DataSet/DVS_npz_events\"\n",
    "frames_dvs_split_path = \"/home/zhang/S2/RP/DataSet/DVS_npz_frames\"\n",
    "visualization_saved_path= \"/home/zhang/S2/RP/DataSet/Visualization_saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aedat_v3(file_name: str) -> Dict:\n",
    "\n",
    "    with open(file_name, 'rb') as bin_f:\n",
    "        # skip ascii header\n",
    "        line = bin_f.readline()\n",
    "        while line.startswith(b'#'):\n",
    "            if line == b'#!END-HEADER\\r\\n':\n",
    "                break\n",
    "            else:\n",
    "                line = bin_f.readline()\n",
    "\n",
    "        txyp = {\n",
    "            't': [],\n",
    "            'x': [],\n",
    "            'y': [],\n",
    "            'p': []\n",
    "        }\n",
    "        while True:\n",
    "            header = bin_f.read(28)\n",
    "            if not header or len(header) == 0:\n",
    "                break\n",
    "\n",
    "            # read header\n",
    "            e_type = struct.unpack('H', header[0:2])[0]\n",
    "            e_source = struct.unpack('H', header[2:4])[0]\n",
    "            e_size = struct.unpack('I', header[4:8])[0]\n",
    "            e_offset = struct.unpack('I', header[8:12])[0]\n",
    "            e_tsoverflow = struct.unpack('I', header[12:16])[0]\n",
    "            e_capacity = struct.unpack('I', header[16:20])[0]\n",
    "            e_number = struct.unpack('I', header[20:24])[0]\n",
    "            e_valid = struct.unpack('I', header[24:28])[0]\n",
    "\n",
    "            data_length = e_capacity * e_size\n",
    "            data = bin_f.read(data_length)\n",
    "            counter = 0\n",
    "\n",
    "            if e_type == 1:\n",
    "                while data[counter:counter + e_size]:\n",
    "                    aer_data = struct.unpack('I', data[counter:counter + 4])[0]\n",
    "                    timestamp = struct.unpack('I', data[counter + 4:counter + 8])[0] | e_tsoverflow << 31\n",
    "                    x = (aer_data >> 17) & 0x00007FFF\n",
    "                    y = (aer_data >> 2) & 0x00007FFF\n",
    "                    pol = (aer_data >> 1) & 0x00000001\n",
    "                    counter = counter + e_size\n",
    "                    txyp['x'].append(x)\n",
    "                    txyp['y'].append(y)\n",
    "                    txyp['t'].append(timestamp)\n",
    "                    txyp['p'].append(pol)\n",
    "            else:\n",
    "                # non-polarity event packet, not implemented\n",
    "                pass\n",
    "        txyp['x'] = np.asarray(txyp['x'])\n",
    "        txyp['y'] = np.asarray(txyp['y'])\n",
    "        txyp['t'] = np.asarray(txyp['t'])\n",
    "        txyp['p'] = np.asarray(txyp['p'])\n",
    "        return txyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [ 72406439  72406447  72406503 ... 192326082 192326339 192326718]\n",
      "x [46 49 45 ... 46 89 64]\n",
      "y [79 96 69 ... 29 14 46]\n",
      "p [1 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#function test\n",
    "d = load_aedat_v3(os.path.join(testData_path,\"user01_fluorescent_led.aedat\"))\n",
    "for i in d.keys():\n",
    "    print(i,d[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_aedat_files_to_npz(fname: str, aedat_file: str, csv_file: str, output_dir: str):\n",
    "    events = load_aedat_v3(aedat_file)\n",
    "    print(f'Start to split [{aedat_file}] to samples.')\n",
    "    # read csv file and get time stamp and label of each sample\n",
    "    # then split the origin data to samples\n",
    "    csv_data = np.loadtxt(csv_file, dtype=np.uint32, delimiter=',', skiprows=1)\n",
    "\n",
    "    # Note that there are some files that many samples have the same label, e.g., user26_fluorescent_labels.csv\n",
    "    file_num = [0] * 11\n",
    "\n",
    "    for i in range(csv_data.shape[0]):\n",
    "        # the label of DVS128 Gesture is 1, 2, ..., 11. We set 0 as the first label, rather than 1\n",
    "        label = csv_data[i][0] - 1\n",
    "        t_start = csv_data[i][1]\n",
    "        t_end = csv_data[i][2]\n",
    "        mask = np.logical_and(events['t'] >= t_start, events['t'] < t_end)\n",
    "        output_file_name = os.path.join(output_dir, str(label), f'{fname}_{file_num[label]}.npz')\n",
    "        np.savez(output_file_name,\n",
    "                    t=events['t'][mask],\n",
    "                    x=events['x'][mask],\n",
    "                    y=events['y'][mask],\n",
    "                    p=events['p'][mask]\n",
    "                    )\n",
    "        print(f'[{output_file_name}] saved.')\n",
    "        file_num[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function test\n",
    "split_aedat_files_to_npz(\"user26_fluorescent\",os.path.join(testData_path,\"user26_fluorescent.aedat\",),os.path.join(testData_path,\"user26_fluorescent_labels.csv\"),os.path.join(testData_path,\"outputTest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_aedat_to_npz(origin_dataset_path: str, events_npz_path: str):\n",
    "\n",
    "    train_dir = os.path.join(events_npz_path, 'train')\n",
    "    test_dir = os.path.join(events_npz_path, 'test')\n",
    "    max_threads_number_for_datasets_preprocess = 16  #my computer has 16 threads \n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(test_dir)\n",
    "    print(f'Mkdir [{train_dir, test_dir}].')\n",
    "    for label in range(11):\n",
    "        os.mkdir(os.path.join(train_dir, str(label)))\n",
    "        os.mkdir(os.path.join(test_dir, str(label)))\n",
    "    print(f'Mkdir {os.listdir(train_dir)} in [{train_dir}] and {os.listdir(test_dir)} in [{test_dir}].')\n",
    "\n",
    "    with open(os.path.join(origin_dataset_path, 'trials_to_train.txt')) as trials_to_train_txt, open(\n",
    "            os.path.join(origin_dataset_path, 'trials_to_test.txt')) as trials_to_test_txt:\n",
    "        # use multi-thread to accelerate\n",
    "        t_ckp = time.time()\n",
    "        with ThreadPoolExecutor(max_workers=min(multiprocessing.cpu_count(), max_threads_number_for_datasets_preprocess)) as tpe:\n",
    "            print(f'Start the ThreadPoolExecutor with max workers = [{tpe._max_workers}].')\n",
    "\n",
    "            for fname in trials_to_train_txt.readlines():\n",
    "                fname = fname.strip()\n",
    "                if fname.__len__() > 0:  #we can see in the files some lines are empty\n",
    "                    aedat_file = os.path.join(origin_dataset_path, fname)\n",
    "                    fname = os.path.splitext(fname)[0] #remove .aedat\n",
    "                    tpe.submit(split_aedat_files_to_npz, fname, aedat_file, os.path.join(origin_dataset_path, fname + '_labels.csv'), train_dir)\n",
    "\n",
    "            for fname in trials_to_test_txt.readlines():\n",
    "                fname = fname.strip()\n",
    "                if fname.__len__() > 0:\n",
    "                    aedat_file = os.path.join(origin_dataset_path, fname)\n",
    "                    fname = os.path.splitext(fname)[0]  #remove .aedat\n",
    "                    tpe.submit(split_aedat_files_to_npz, fname, aedat_file,\n",
    "                                os.path.join(origin_dataset_path, fname + '_labels.csv'), test_dir)\n",
    "\n",
    "        print(f'Used time = [{round(time.time() - t_ckp, 2)}s].')\n",
    "    print(f'All aedat files have been split to samples and saved into [{train_dir, test_dir}].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_aedat_to_npz(dvs_path,events_dvs_split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [180122762 180122794 180122807 ... 188583498 188583504 188583508]\n",
      "x [52 91 45 ... 45 51 54]\n",
      "y [ 85  79  90 ... 108 113 115]\n",
      "p [1 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Check file classified\n",
    "r = np.load(os.path.join(events_dvs_split_path,\"test/9/user29_lab_0.npz\"))\n",
    "for i in r.keys():\n",
    "    print(i,r[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_set_events(npz_dir: str):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    train_label = []\n",
    "    test_label = []\n",
    "\n",
    "    npz_dir_list = os.listdir(npz_dir) #dir train,test\n",
    "    for dir0 in npz_dir_list:\n",
    "        dir1 = os.path.join(npz_dir,dir0) \n",
    "        classes_dir = os.listdir(dir1) #dir 0,1,2...\n",
    "        for dir2 in classes_dir:\n",
    "            file_dir = os.path.join(dir1,dir2) \n",
    "            filrs_names = os.listdir(file_dir)\n",
    "            for file_name in filrs_names:\n",
    "                npz_file = np.load(os.path.join(file_dir,file_name))\n",
    "                t=npz_file['t']\n",
    "                x=npz_file['x']\n",
    "                y=npz_file['y']\n",
    "                p=npz_file['p']\n",
    "                txyp = {'t': t,'x': x,'y': y,'p': p}\n",
    "                if dir0 ==\"train\":\n",
    "                    train_data.append(txyp)\n",
    "                    train_label.append(int(dir2)) \n",
    "                else:\n",
    "                    test_data.append(txyp)\n",
    "                    test_label.append(int(dir2))\n",
    "                    \n",
    "    return np.array(train_data), np.array(train_label), np.array(test_data), np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_label,test_data,test_label = create_data_set_events(events_dvs_split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1176 samples in the training set\n",
      "We have 288 samples in the test set\n",
      "Total samples: 1464\n"
     ]
    }
   ],
   "source": [
    "num_train = len(train_data)\n",
    "num_test = len(test_data)\n",
    "print(f'We have {num_train} samples in the training set')\n",
    "print(f'We have {num_test} samples in the test set')\n",
    "print(f'Total samples: {num_test+num_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [66802955 66802957 66802972 ... 71631530 71631566 71631587]\n",
      "x [91 83 85 ... 83 85 81]\n",
      "y [ 66 102  69 ...  86  82  71]\n",
      "p [1 1 1 ... 1 1 1]\n",
      "label 2\n"
     ]
    }
   ],
   "source": [
    "#Check dataset split\n",
    "events,label = train_data[25],train_label[25]\n",
    "for k in events.keys():\n",
    "    print(k,events[k])\n",
    "print('label',label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "## Convert events stream to frames stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_same_directory_structure(source_dir: str, target_dir: str):\n",
    "    for sub_dir_name in os.listdir(source_dir):\n",
    "        source_sub_dir = os.path.join(source_dir, sub_dir_name)\n",
    "        if os.path.isdir(source_sub_dir):\n",
    "            target_sub_dir = os.path.join(target_dir, sub_dir_name)\n",
    "            os.mkdir(target_sub_dir)\n",
    "            print(f'Mkdir [{target_sub_dir}].')\n",
    "            create_same_directory_structure(source_sub_dir, target_sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create same structure for frames like events above for each type of spliting\n",
    "create_same_directory_structure(events_dvs_split_path, \"/home/zhang/S2/RP/DataSet/DVS_npz_frames/split_by_number/\")\n",
    "create_same_directory_structure(events_dvs_split_path, \"/home/zhang/S2/RP/DataSet/DVS_npz_frames/split_by_time/\")\n",
    "create_same_directory_structure(events_dvs_split_path, \"/home/zhang/S2/RP/DataSet/DVS_npz_frames/split_by_fixed_duration/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote `frames_num` as `T`, if `split_by` is `'time'`, then\n",
    "$$\\Delta T = [\\frac{t_{N-1} - t_{0}}{T}]$$\n",
    "$$j_{l} = \\mathop{\\arg\\min}\\limits_{k} {t_{k} | t_{k} \\geq t_{0} + \\Delta T \\cdot j}$$\n",
    "$$j_{r} = \\begin{cases} \\mathop{\\arg\\max}\\limits_{k} \\{t_{k} | t_{k} < t_{0} + \\Delta T \\cdot (j + 1)\\} + 1, & j <  T - 1 \\cr N, & j = T - 1 \\end{cases}$$\n",
    "If ``split_by`` is ``'number'``, then\n",
    "\n",
    "$$j_{l} = [\\frac{N}{T}] \\cdot j $$\n",
    "$$j_{r} = \\begin{cases} [\\frac{N}{T}] \\cdot (j + 1), & j <  T - 1 \\cr N, & j = T - 1 \\end{cases}$$\n",
    "\n",
    "Where `T` is number of frames split, `j` is index of frame, `N` is number of events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote a two channels frame as :\n",
    "\n",
    "A frame in the frame data after integration noted `F(j)` and a pixel at $(p, x, y)$ as $F(j, p, x, y)$, the pixel value is integrated from the events data whose indices are in $[j_{l}, j_{r})$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "j_l & =\\left\\lfloor\\frac{N}{T}\\right\\rfloor \\cdot j \\\\\n",
    "j_r & = \\begin{cases}\\left\\lfloor\\frac{N}{T}\\right\\rfloor \\cdot(j+1), & \\text { if } j<T-1 \\\\\n",
    "N, & \\text { if } j=T-1\\end{cases} \\\\\n",
    "F(j, p, x, y) & =\\sum_{i=j_l}^{j_r-1} \\mathcal{I}_{p, x, y}\\left(p_i, x_i, y_i\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\lfloor$ $\\cdot$ $\\rfloor$ is the floor operation, T is number of frames split.\n",
    "$$\\mathcal{I}_{p, x, y}(p_{i}, x_{i}, y_{i})$$ \n",
    "is an indicator function and it equals 1 only when $$(p, x, y) = (p_{i}, x_{i}, y_{i})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_events_segment_to_frame(x: np.ndarray, y: np.ndarray, p: np.ndarray, H: int, W: int, j_l: int, j_r: int) -> np.ndarray:\n",
    "    frame = np.zeros(shape=[2, H * W])\n",
    "    x = x[j_l: j_r].astype(int)  # avoid overflow\n",
    "    y = y[j_l: j_r].astype(int)\n",
    "    p = p[j_l: j_r]\n",
    "    mask = []\n",
    "    mask.append(p == 0)\n",
    "    mask.append(np.logical_not(mask[0]))\n",
    "    for c in range(2):\n",
    "        position = y[mask[c]] * W + x[mask[c]]\n",
    "        events_number_per_pos = np.bincount(position)\n",
    "        frame[c][np.arange(events_number_per_pos.size)] += events_number_per_pos\n",
    "    return frame.reshape((2, H, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixed_frames_number_segment_index(events_t: np.ndarray, split_by: str, frames_num: int) -> tuple:\n",
    "    j_l = np.zeros(shape=[frames_num], dtype=int)\n",
    "    j_r = np.zeros(shape=[frames_num], dtype=int)\n",
    "    N = events_t.size\n",
    "\n",
    "    if split_by == 'number':\n",
    "        di = N // frames_num\n",
    "        for i in range(frames_num):\n",
    "            j_l[i] = i * di\n",
    "            j_r[i] = j_l[i] + di\n",
    "        j_r[-1] = N\n",
    "\n",
    "    elif split_by == 'time':\n",
    "        dt = (events_t[-1] - events_t[0]) // frames_num\n",
    "        idx = np.arange(N)\n",
    "        for i in range(frames_num):\n",
    "            t_l = dt * i + events_t[0]\n",
    "            t_r = t_l + dt\n",
    "            mask = np.logical_and(events_t >= t_l, events_t < t_r)\n",
    "            idx_masked = idx[mask]\n",
    "            j_l[i] = idx_masked[0]\n",
    "            j_r[i] = idx_masked[-1] + 1\n",
    "\n",
    "        j_r[-1] = N\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return j_l, j_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_events_by_fixed_frames_number(events: Dict, label: str, split_by: str, frames_num: int, H: int, W: int) -> np.ndarray:\n",
    "    '''\n",
    "    :param events: a dict whose keys are ``['t', 'x', 'y', 'p']`` and values are ``numpy.ndarray``\n",
    "    :param split_by: 'time' or 'number'\n",
    "    :param frames_num: the number of frames\n",
    "    :param H: the height of frame\n",
    "    :param W: the weight of frame\n",
    "    :return: frames,label\n",
    "    Integrate events to frames by fixed frames number.\n",
    "    '''\n",
    "    t, x, y, p = (events[key] for key in ('t', 'x', 'y', 'p'))\n",
    "    j_l, j_r = get_fixed_frames_number_segment_index(t, split_by, frames_num)\n",
    "    frames = np.zeros([frames_num, 2, H, W])\n",
    "    for i in range(frames_num):\n",
    "        frames[i] = integrate_events_segment_to_frame(x, y, p, H, W, j_l[i], j_r[i])\n",
    "    return frames,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2, 128, 128)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Test function\n",
    "events,label = train_data[500],train_label[500]\n",
    "frames,label = integrate_events_by_fixed_frames_number(events=events,label=label, split_by='number',frames_num=20,H=128,W=128)\n",
    "print(frames.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0\n"
     ]
    }
   ],
   "source": [
    "f = frames.flatten()\n",
    "print(max(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_events_by_fixed_duration(events: Dict, duration: int, H: int, W: int) -> np.ndarray:\n",
    "    '''\n",
    "    Integrate events to frames by fixed time duration of each frame.\n",
    "    '''\n",
    "    x = events['x']\n",
    "    y = events['y']\n",
    "    t = events['t']\n",
    "    p = events['p']\n",
    "    N = t.size\n",
    "\n",
    "    frames = []\n",
    "    left = 0\n",
    "    right = 0\n",
    "    while True:\n",
    "        t_l = t[left]\n",
    "        while True:\n",
    "            if right == N or t[right] - t_l > duration:\n",
    "                break\n",
    "            else:\n",
    "                right += 1\n",
    "        # integrate from index [left, right)\n",
    "        frames.append(np.expand_dims(integrate_events_segment_to_frame(x, y, p, H, W, left, right), 0))\n",
    "\n",
    "        left = right\n",
    "\n",
    "        if right == N:\n",
    "            return np.concatenate(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_zeros_with_variable_length(X):\n",
    "    x_list = []\n",
    "    x_len_list = []\n",
    "    for x  in X:\n",
    "        x_list.append(torch.as_tensor(x))\n",
    "        x_len_list.append(x.shape[0])\n",
    "\n",
    "    return torch.nn.utils.rnn.pad_sequence(x_list, batch_first=True).numpy(), np.array(x_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frame_to_gif(frames: torch.Tensor or np.ndarray, label:str, save_path: str,unique_ = \"\") -> None:\n",
    "    '''\n",
    "    frames: frames with `shape=[T, 2, H, W]`\n",
    "         save frames to a gif file in the directory `save_path`\n",
    "    '''\n",
    "    if isinstance(frames, np.ndarray):\n",
    "        frames = torch.from_numpy(frames)\n",
    "    to_img = transforms.ToPILImage()\n",
    "    img_tensor = torch.zeros([frames.shape[0], 3, frames.shape[2], frames.shape[3]])\n",
    "    img_tensor[:, 1] = frames[:, 0]\n",
    "    img_tensor[:, 2] = frames[:, 1]\n",
    "\n",
    "    gif_frames = []\n",
    "    for t in range(img_tensor.shape[0]):\n",
    "        img = to_img(img_tensor[t])\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Visualization frame_{t}\")\n",
    "        plt.savefig(os.path.join(save_path,f\"frame_{t}.png\"))\n",
    "        plt.close()\n",
    "        img_read = imageio.v2.imread(os.path.join(save_path,f\"frame_{t}.png\"))\n",
    "        gif_frames.append(img_read)\n",
    "\n",
    "    imageio.mimsave(os.path.join(save_path,f\"visual_label_{label}\"+f\"{unique_}.gif\"), # output gif\n",
    "            gif_frames,          # array of input frames\n",
    "            fps = 10)         # frames per second\n",
    "    \n",
    "    for file in os.listdir(save_path):  #remove png generated\n",
    "        if(file[:5]==\"frame\"):\n",
    "            os.remove(os.path.join(save_path,file))\n",
    "\n",
    "    print(f'Save gif to [{save_path+f\"/visual_label_{label}\"+f\"{unique_}.gif\"}].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each class we save a gif example\n",
    "class_diff_index_list = [0,98,196,294,392,490,686,784,883,1000,1078]\n",
    "for i in class_diff_index_list:\n",
    "    events_,label_ = train_data[i],train_label[i]\n",
    "    frames_,label_ = integrate_events_by_fixed_frames_number(events=events_,label=label_, split_by='number',frames_num=20,H=128,W=128)\n",
    "    save_frame_to_gif(frames_,label_,visualization_saved_path,unique_=\"_num_2222\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_same_directory_structure(frames_dvs_split_path,visualization_saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_events_data_to_frame_to_dir(events_data: np.ndarray, events_data_label: np.ndarray,data_type:str, npz_save_path_root: str, split_by: str, frames_num: int, H: int, W: int, visual_save_path_root =None):\n",
    "    num_data = len(events_data)\n",
    "    for i in range(num_data):\n",
    "        event_,label_ = events_data[i],events_data_label[i]\n",
    "        frames_, label_ = integrate_events_by_fixed_frames_number(events=event_,label=label_,split_by=split_by,frames_num=frames_num,H=H,W=W)\n",
    "        if visual_save_path_root is not None:\n",
    "            save_frame_to_gif(frames_,label_,os.path.join(visual_save_path_root,data_type,str(label_)),unique_=\"_num_\"+str(i))\n",
    "        np.save(os.path.join(npz_save_path_root,data_type,str(label_),str(i)),frames_)\n",
    "        print(f\"save npy to [{os.path.join(npz_save_path_root,data_type,str(label_),str(i))}.npy]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_events_data_to_frame_npz_by_fixed_frames_number(events_train_data: np.ndarray, events_train_label: np.ndarray, events_test_data: np.ndarray, events_test_label : np.ndarray, npz_save_path_root: str, split_by: str, frames_num: int, H: int, W: int, visual_save_path =None)-> None:\n",
    "    assert frames_num > 0 and isinstance(frames_num, int)\n",
    "    assert split_by == 'time' or split_by == 'number' or split_by == \"fixed_duration\"\n",
    "    if split_by==\"number\":\n",
    "        npz_root_path = os.path.join(npz_save_path_root,\"split_by_number\")\n",
    "    elif split_by==\"time\":\n",
    "        npz_root_path = os.path.join(npz_save_path_root,\"split_by_time\")\n",
    "\n",
    "    if visual_save_path is not None:\n",
    "        if split_by==\"number\":\n",
    "            visual_save_path = os.path.join(visual_save_path,\"split_by_number\")\n",
    "        elif split_by==\"time\":\n",
    "            visual_save_path = os.path.join(visual_save_path,\"split_by_time\")\n",
    "\n",
    "    t_ckp = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=min(multiprocessing.cpu_count(), 16)) as tpe:\n",
    "        print(f'Start the ThreadPoolExecutor with max workers = [{tpe._max_workers}].')\n",
    "        tpe.submit(save_events_data_to_frame_to_dir,events_train_data,events_train_label,\"train\",npz_root_path,split_by,frames_num,H,W,visual_save_path)\n",
    "        tpe.submit(save_events_data_to_frame_to_dir,events_test_data,events_test_label,\"test\",npz_root_path,split_by,frames_num,H,W,visual_save_path)\n",
    "    #save_events_data_to_frame_to_dir(events_train_data,events_train_label,\"train\",npz_root_path,split_by,frames_num,H,W,visual_save_path)\n",
    "    #save_events_data_to_frame_to_dir(events_test_data,events_test_label,\"test\",npz_root_path,split_by,frames_num,H,W,visual_save_path)\n",
    "\n",
    "    print(f'Used time = [{round(time.time() - t_ckp, 2)}s].')\n",
    "    print(f'All files have been saved.')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_events_data_to_frame_npy_by_fixed_time_duration(events_train_data: np.ndarray, events_train_label: np.ndarray, events_test_data: np.ndarray, events_test_label : np.ndarray, npy_save_path_root: str,durantion:int, H: int, W: int, visual_save_path =None)-> np.ndarray:\n",
    "    len_train = len(events_train_data)\n",
    "    len_test = len(events_test_data)\n",
    "    data = np.concatenate((events_train_data,events_test_data))\n",
    "    data_label = np.concatenate((events_train_label,events_test_label))\n",
    "    train_save_path = os.path.join(npy_save_path_root,\"split_by_fixed_duration\",\"train\")\n",
    "    test_save_path = os.path.join(npy_save_path_root,\"split_by_fixed_duration\",\"test\")\n",
    "    frames_variable_len_list = []\n",
    "\n",
    "    for e in data:\n",
    "        frames_variable_len_list.append(integrate_events_by_fixed_duration(e,durantion,H,W)) \n",
    "    frames_padding,frames_origin_len_list = padding_zeros_with_variable_length(frames_variable_len_list)\n",
    "    t_ckp = time.time()\n",
    "\n",
    "    for i in range(len_train):\n",
    "        np.save(os.path.join(train_save_path,str(data_label[i]),str(i)),frames_padding[i])\n",
    "        print(f\"save npy to [{os.path.join(train_save_path,str(data_label[i]),str(i))}.npy]\") \n",
    "        if visual_save_path is not None:\n",
    "            save_frame_to_gif(frames_padding[i],data_label[i],os.path.join(visual_save_path,\"split_by_fixed_duration\",\"train\",str(data_label[i])),unique_=\"_num_\"+str(i))\n",
    "    for i in range(len_train,len_train+len_test):\n",
    "        np.save(os.path.join(test_save_path,str(data_label[i]),str(i)),frames_padding[i])\n",
    "        print(f\"save npy to [{os.path.join(test_save_path,str(data_label[i]),str(i))}.npy]\")\n",
    "        if visual_save_path is not None:\n",
    "            save_frame_to_gif(frames_padding[i],data_label[i],os.path.join(visual_save_path,\"split_by_fixed_duration\",\"test\",str(data_label[i])),unique_=\"_num_\"+str(i))\n",
    "\n",
    "    print(f'Used time = [{round(time.time() - t_ckp, 2)}s].')\n",
    "    print(f'All files have been saved.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_events_data_to_frame_npz_by_fixed_frames_number(train_data,train_label,test_data,test_label,frames_dvs_split_path,\"number\",20,128,128,visual_save_path = None)\n",
    "save_events_data_to_frame_npz_by_fixed_frames_number(train_data,train_label,test_data,test_label,frames_dvs_split_path,\"time\",20,128,128,visual_save_path = None)\n",
    "save_events_data_to_frame_npy_by_fixed_time_duration(train_data,train_label,test_data,test_label,frames_dvs_split_path,500000,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_set_frames(npy_dir: str):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    train_label = []\n",
    "    test_label = []\n",
    "\n",
    "    npz_dir_list = os.listdir(npy_dir) #dir train,test\n",
    "    for dir0 in npz_dir_list:\n",
    "        dir1 = os.path.join(npy_dir,dir0) \n",
    "        classes_dir = os.listdir(dir1) #dir 0,1,2...\n",
    "        for dir2 in classes_dir:\n",
    "            file_dir = os.path.join(dir1,dir2) \n",
    "            filrs_names = os.listdir(file_dir)\n",
    "            for file_name in filrs_names:\n",
    "                npy = np.load(os.path.join(file_dir,file_name))\n",
    "                if dir0 ==\"train\":\n",
    "                    train_data.append(npy)\n",
    "                    train_label.append(int(dir2)) \n",
    "                else:\n",
    "                    test_data.append(npy)\n",
    "                    test_label.append(int(dir2))\n",
    "                    \n",
    "    return np.array(train_data), np.array(train_label), np.array(test_data), np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_frames, train_label_frames, test_data_frames, test_label_frames = create_data_set_frames(os.path.join(frames_dvs_split_path,\"split_by_fixed_duration\"))\n",
    "save_path=os.path.join(frames_dvs_split_path,\"split_by_fixed_duration\")\n",
    "np.save(os.path.join(save_path,\"train_data_frames_fixed_duration\"),np.float32(train_data_frames))\n",
    "np.save(os.path.join(save_path,\"train_label_frames_fixed_duration\"),np.float32(train_label_frames))\n",
    "np.save(os.path.join(save_path,\"test_data_frames_fixed_duration\"),np.float32(test_data_frames))\n",
    "np.save(os.path.join(save_path,\"test_label_frames_fixed_duration\"),np.float32(test_label_frames))\n",
    "\n",
    "train_data_frames, train_label_frames, test_data_frames, test_label_frames = create_data_set_frames(os.path.join(frames_dvs_split_path,\"split_by_number\"))\n",
    "save_path=os.path.join(frames_dvs_split_path,\"split_by_number\")\n",
    "np.save(os.path.join(save_path,\"train_data_frames_number\"),np.float32(train_data_frames))\n",
    "np.save(os.path.join(save_path,\"train_label_frames_number\"),np.float32(train_label_frames))\n",
    "np.save(os.path.join(save_path,\"test_data_frames_number\"),np.float32(test_data_frames))\n",
    "np.save(os.path.join(save_path,\"test_label_frames_number\"),np.float32(test_label_frames))\n",
    "\n",
    "train_data_frames, train_label_frames, test_data_frames, test_label_frames = create_data_set_frames(os.path.join(frames_dvs_split_path,\"split_by_time\"))\n",
    "save_path=os.path.join(frames_dvs_split_path,\"split_by_time\")\n",
    "np.save(os.path.join(save_path,\"train_data_frames_time\"),np.float32(train_data_frames))\n",
    "np.save(os.path.join(save_path,\"train_label_frames_time\"),np.float32(train_label_frames))\n",
    "np.save(os.path.join(save_path,\"test_data_frames_time\"),np.float32(test_data_frames))\n",
    "np.save(os.path.join(save_path,\"test_label_frames_time\"),np.float32(test_label_frames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"/home/zhang/S2/RP/DataSet/npyFinalDataset/train_data_frames_fixed_duration.npy\")\n",
    "y_train = np.load(\"/home/zhang/S2/RP/DataSet/npyFinalDataset/train_label_frames_fixed_duration.npy\")\n",
    "\n",
    "X_test = np.load(\"/home/zhang/S2/RP/DataSet/npyFinalDataset/test_data_frames_fixed_duration.npy\")\n",
    "y_test = np.load(\"/home/zhang/S2/RP/DataSet/npyFinalDataset/test_label_frames_fixed_duration.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1176, 37, 2, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1425801216,)\n",
      "236.0\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(x[1044450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.0\n"
     ]
    }
   ],
   "source": [
    "t = x[:10000]\n",
    "print(max(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
